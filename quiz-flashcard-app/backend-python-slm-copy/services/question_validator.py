"""
Question Validator Service - Scores and validates generated questions.

This service validates questions generated by the GenerationAgent,
scoring them on 8 quality dimensions and filtering out low-quality ones.

Phase 3 of RAG Pipeline Implementation.
"""
import json
import re
from typing import Any, Dict, List, Optional, Tuple

import structlog
from sqlalchemy.ext.asyncio import AsyncSession

from services.ai_service import ai_service

logger = structlog.get_logger()

# 8-dimension quality scoring weights (research-backed)
# Same weights as analysis_agent.py for consistency
QUALITY_WEIGHTS = {
    "clarity": 0.15,           # Clear, unambiguous wording
    "content_accuracy": 0.20,  # Factually correct content
    "answer_accuracy": 0.15,   # Correct answer is truly correct
    "distractor_quality": 0.15,  # Plausible but incorrect options
    "cognitive_level": 0.10,   # Appropriate Bloom's level
    "rationale_quality": 0.10, # Helpful explanation
    "single_concept": 0.10,    # Tests one concept only
    "cover_test": 0.05,        # Can't be answered without content
}

# Quality thresholds
HIGH_QUALITY_THRESHOLD = 4.0   # Used for few-shot examples
MINIMUM_ACCEPTABLE = 3.5       # Below this, question is rejected
REFINEMENT_THRESHOLD = 4.0    # Below this, question needs refinement

VALIDATION_SYSTEM_PROMPT = """You are an expert educational content reviewer specializing in question quality assessment.

Your task is to evaluate generated questions on 8 quality dimensions and provide actionable feedback.

Quality Dimensions (1=poor, 5=excellent):
1. clarity: Clear, unambiguous wording with no confusing phrasing
2. content_accuracy: All factual content is correct
3. answer_accuracy: The marked correct answer is truly and unambiguously correct
4. distractor_quality: Wrong options are plausible but clearly incorrect
5. cognitive_level: Tests appropriate thinking level (Bloom's taxonomy)
6. rationale_quality: Explanation is helpful, educational, and teaches the concept
7. single_concept: Tests only one concept (not multiple packed together)
8. cover_test: Question can't be answered without knowing the source material

Be strict but fair. A score of 5 means exceptional quality, 3 is acceptable, below 3 has issues."""


class QuestionValidator:
    """
    Service for validating and scoring generated questions.

    Scores questions on 8 quality dimensions and can refine
    low-scoring questions to improve quality.
    """

    def __init__(
        self,
        minimum_score: float = MINIMUM_ACCEPTABLE,
        refinement_threshold: float = REFINEMENT_THRESHOLD,
    ):
        """
        Initialize question validator.

        Args:
            minimum_score: Questions below this score are rejected
            refinement_threshold: Questions below this score need refinement
        """
        self.minimum_score = minimum_score
        self.refinement_threshold = refinement_threshold

    async def validate_questions(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Validate and score a batch of questions.

        Args:
            questions: List of question dicts from GenerationAgent
            quality_criteria: Optional criteria from AnalysisAgent
            source_content: Optional source material for accuracy checking

        Returns:
            Tuple of (passed_questions, failed_questions)
        """
        if not questions:
            return [], []

        logger.info(
            "validating_questions",
            count=len(questions),
            has_quality_criteria=quality_criteria is not None,
        )

        # Score all questions
        scored = await self._score_batch(questions, quality_criteria, source_content)

        # Separate passed and failed
        passed = []
        failed = []

        for q in scored:
            overall = q.get("quality_score", 0)
            if overall >= self.minimum_score:
                passed.append(q)
            else:
                failed.append(q)

        logger.info(
            "validation_complete",
            total=len(questions),
            passed=len(passed),
            failed=len(failed),
            avg_score=sum(q.get("quality_score", 0) for q in scored) / len(scored) if scored else 0,
        )

        return passed, failed

    async def validate_and_refine(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
        max_refinement_attempts: int = 1,
    ) -> List[Dict[str, Any]]:
        """
        Validate questions and attempt to refine low-scoring ones.

        Args:
            questions: List of question dicts
            quality_criteria: Optional criteria from AnalysisAgent
            source_content: Optional source material
            max_refinement_attempts: Max attempts to refine each question

        Returns:
            List of validated (and possibly refined) questions
        """
        passed, failed = await self.validate_questions(
            questions, quality_criteria, source_content
        )

        # Attempt to refine questions that are close to passing
        refinable = [
            q for q in failed
            if q.get("quality_score", 0) >= self.minimum_score - 0.5
        ]

        if refinable and max_refinement_attempts > 0:
            logger.info("refining_questions", count=len(refinable))
            refined = await self._refine_batch(
                refinable, quality_criteria, source_content
            )
            # Re-validate refined questions
            refined_passed, _ = await self.validate_questions(
                refined, quality_criteria, source_content
            )
            passed.extend(refined_passed)

        return passed

    async def _score_batch(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """Score a batch of questions using AI."""
        prompt = self._build_scoring_prompt(questions, quality_criteria, source_content)

        try:
            response = await ai_service.generate_text(
                prompt=prompt,
                system_prompt=VALIDATION_SYSTEM_PROMPT,
                max_tokens=4000,
            )
            return self._parse_scoring_response(response, questions)
        except Exception as e:
            logger.error("scoring_batch_failed", error=str(e))
            # Return with default scores
            return [
                {**q, "quality_score": 3.0, "quality_scores": self._default_scores()}
                for q in questions
            ]

    def _build_scoring_prompt(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
    ) -> str:
        """Build the scoring prompt."""
        parts = []

        # Add quality criteria context if available
        if quality_criteria:
            summary = quality_criteria.get("analysis_summary", {})
            if summary:
                parts.append(f"""Quality Context:
- Target average quality: {summary.get('average_quality', 4.0)}/5
- Emphasize: {summary.get('strongest_dimension', 'clarity')}
- Watch for issues in: {summary.get('weakest_dimension', 'distractor_quality')}
""")

        # Add source content snippet if available
        if source_content:
            content_preview = source_content[:1000] + "..." if len(source_content) > 1000 else source_content
            parts.append(f"""Source Material (for accuracy checking):
{content_preview}
""")

        # Add questions to score
        parts.append(f"Score these {len(questions)} generated questions:\n")

        for i, q in enumerate(questions, 1):
            q_text = f"""
--- Question {i} ---
Type: {q.get('question_type', 'multiple_choice')}
Question: {q.get('question_text', '')}"""

            if q.get('options'):
                q_text += f"\nOptions: {q['options']}"

            q_text += f"\nCorrect Answer: {q.get('correct_answer', '')}"

            if q.get('explanation'):
                q_text += f"\nExplanation: {q['explanation']}"

            parts.append(q_text)

        parts.append("""

For each question, provide scores on all 8 dimensions and an overall weighted score.

Return JSON:
{
    "scored_questions": [
        {
            "question_index": 1,
            "quality_scores": {
                "clarity": 4,
                "content_accuracy": 5,
                "answer_accuracy": 5,
                "distractor_quality": 4,
                "cognitive_level": 4,
                "rationale_quality": 4,
                "single_concept": 5,
                "cover_test": 4
            },
            "overall_score": 4.35,
            "bloom_level": "apply",
            "issues": ["List any issues found"],
            "suggestions": ["List improvement suggestions"]
        }
    ]
}""")

        return "\n".join(parts)

    def _parse_scoring_response(
        self,
        response: str,
        original_questions: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Parse scoring response and merge with original questions."""
        cleaned = self._clean_json_response(response)

        try:
            data = json.loads(cleaned)
            scored_list = data.get("scored_questions", [])

            # Create mapping by index
            score_map = {}
            for item in scored_list:
                idx = item.get("question_index", 0) - 1  # Convert to 0-based
                score_map[idx] = item

            # Merge scores into original questions
            result = []
            for i, q in enumerate(original_questions):
                scores_data = score_map.get(i, {})
                quality_scores = scores_data.get("quality_scores", self._default_scores())
                overall = scores_data.get("overall_score") or self._calculate_overall(quality_scores)

                q_with_scores = {
                    **q,
                    "quality_scores": quality_scores,
                    "quality_score": overall,  # For consistency with Question model
                    "bloom_level": scores_data.get("bloom_level", "understand"),
                    "validation_issues": scores_data.get("issues", []),
                    "validation_suggestions": scores_data.get("suggestions", []),
                }
                result.append(q_with_scores)

            return result

        except json.JSONDecodeError:
            logger.warning("scoring_parse_failed", response_preview=response[:200])
            return [
                {**q, "quality_score": 3.0, "quality_scores": self._default_scores()}
                for q in original_questions
            ]

    async def _refine_batch(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """Attempt to refine low-quality questions."""
        prompt = self._build_refinement_prompt(questions, quality_criteria, source_content)

        try:
            response = await ai_service.generate_text(
                prompt=prompt,
                system_prompt=VALIDATION_SYSTEM_PROMPT,
                max_tokens=4000,
            )
            return self._parse_refinement_response(response, questions)
        except Exception as e:
            logger.error("refinement_failed", error=str(e))
            return questions  # Return originals if refinement fails

    def _build_refinement_prompt(
        self,
        questions: List[Dict[str, Any]],
        quality_criteria: Optional[Dict] = None,
        source_content: Optional[str] = None,
    ) -> str:
        """Build the refinement prompt."""
        parts = ["These questions scored below threshold and need improvement:\n"]

        for i, q in enumerate(questions, 1):
            issues = q.get("validation_issues", [])
            suggestions = q.get("validation_suggestions", [])

            parts.append(f"""
--- Question {i} (Score: {q.get('quality_score', 0):.2f}) ---
Original: {q.get('question_text', '')}
Options: {q.get('options', [])}
Answer: {q.get('correct_answer', '')}
Issues: {issues}
Suggestions: {suggestions}
""")

        parts.append("""
Improve each question to address the identified issues.
Keep the core concept but improve clarity, accuracy, and distractors.

Return JSON:
{
    "refined_questions": [
        {
            "question_index": 1,
            "question_text": "Improved question text",
            "options": ["A) ...", "B) ...", "C) ...", "D) ..."],
            "correct_answer": "Correct option text",
            "explanation": "Improved explanation",
            "changes_made": ["List of changes made"]
        }
    ]
}""")

        return "\n".join(parts)

    def _parse_refinement_response(
        self,
        response: str,
        original_questions: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Parse refinement response."""
        cleaned = self._clean_json_response(response)

        try:
            data = json.loads(cleaned)
            refined_list = data.get("refined_questions", [])

            # Create mapping by index
            refined_map = {}
            for item in refined_list:
                idx = item.get("question_index", 0) - 1
                refined_map[idx] = item

            # Merge refined data with originals
            result = []
            for i, q in enumerate(original_questions):
                refined = refined_map.get(i)
                if refined:
                    # Merge refined fields with original
                    updated = {
                        **q,
                        "question_text": refined.get("question_text", q.get("question_text")),
                        "options": refined.get("options", q.get("options")),
                        "correct_answer": refined.get("correct_answer", q.get("correct_answer")),
                        "explanation": refined.get("explanation", q.get("explanation")),
                        "was_refined": True,
                        "refinement_changes": refined.get("changes_made", []),
                    }
                    result.append(updated)
                else:
                    result.append({**q, "was_refined": False})

            return result

        except json.JSONDecodeError:
            logger.warning("refinement_parse_failed", response_preview=response[:200])
            return original_questions

    def _clean_json_response(self, response: str) -> str:
        """Clean AI response to extract JSON."""
        # Remove markdown code blocks
        response = re.sub(r"```json\s*", "", response)
        response = re.sub(r"```\s*", "", response)

        # Find JSON object
        start = response.find("{")
        end = response.rfind("}") + 1

        if start != -1 and end > start:
            return response[start:end]

        return response

    def _default_scores(self) -> Dict[str, float]:
        """Return default quality scores (3.0 = average)."""
        return {dim: 3.0 for dim in QUALITY_WEIGHTS.keys()}

    def _calculate_overall(self, scores: Dict[str, float]) -> float:
        """Calculate weighted overall score."""
        total = 0.0
        for dim, weight in QUALITY_WEIGHTS.items():
            total += scores.get(dim, 3.0) * weight
        return round(total, 2)

    def get_quality_report(
        self,
        questions: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Generate a quality report for a set of scored questions.

        Args:
            questions: List of questions with quality_scores

        Returns:
            Summary report with statistics
        """
        if not questions:
            return {"count": 0, "average_score": 0, "distribution": {}}

        scores = [q.get("quality_score", 0) for q in questions]
        avg = sum(scores) / len(scores)

        # Distribution by quality tier
        distribution = {
            "excellent": sum(1 for s in scores if s >= 4.5),
            "good": sum(1 for s in scores if 4.0 <= s < 4.5),
            "acceptable": sum(1 for s in scores if 3.5 <= s < 4.0),
            "needs_work": sum(1 for s in scores if 3.0 <= s < 3.5),
            "poor": sum(1 for s in scores if s < 3.0),
        }

        # Dimension averages
        dimension_totals = {dim: 0.0 for dim in QUALITY_WEIGHTS.keys()}
        dimension_counts = {dim: 0 for dim in QUALITY_WEIGHTS.keys()}

        for q in questions:
            for dim, score in q.get("quality_scores", {}).items():
                if dim in dimension_totals:
                    dimension_totals[dim] += score
                    dimension_counts[dim] += 1

        dimension_averages = {
            dim: round(dimension_totals[dim] / dimension_counts[dim], 2)
            if dimension_counts[dim] > 0 else 0
            for dim in QUALITY_WEIGHTS.keys()
        }

        # Bloom's taxonomy distribution
        bloom_dist = {}
        for q in questions:
            level = q.get("bloom_level", "unknown")
            bloom_dist[level] = bloom_dist.get(level, 0) + 1

        return {
            "count": len(questions),
            "average_score": round(avg, 2),
            "min_score": round(min(scores), 2),
            "max_score": round(max(scores), 2),
            "distribution": distribution,
            "dimension_averages": dimension_averages,
            "bloom_distribution": bloom_dist,
            "pass_rate": round(
                sum(1 for s in scores if s >= self.minimum_score) / len(scores) * 100, 1
            ),
        }


# Global service instance
question_validator = QuestionValidator()
