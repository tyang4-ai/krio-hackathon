# Server Configuration
PORT=3001
NODE_ENV=development

# =============================================================================
# AI PROVIDER CONFIGURATION
# =============================================================================
# Choose your AI provider: nvidia, groq, together, ollama, bedrock, huggingface
# Default: nvidia
AI_PROVIDER=nvidia

# Optional: Override the default model for your chosen provider
# AI_MODEL=

# AI Service Configuration
AI_MAX_RETRIES=3
AI_RETRY_DELAY=1000

# -----------------------------------------------------------------------------
# NVIDIA Nemotron (Default)
# -----------------------------------------------------------------------------
# Get API key from: https://build.nvidia.com/
# Models: nvidia/llama-3.3-nemotron-super-49b-v1 (default)
NVIDIA_API_KEY=your_nvidia_api_key_here

# -----------------------------------------------------------------------------
# Groq (Recommended for Vercel Deployment)
# -----------------------------------------------------------------------------
# Get API key from: https://console.groq.com/
# Models: mixtral-8x7b-32768 (default), llama-3.1-70b-versatile, llama-3.1-8b-instant
# Pros: Fastest inference, generous free tier, great for production
# GROQ_API_KEY=your_groq_api_key_here

# -----------------------------------------------------------------------------
# Together.ai (Good Balance)
# -----------------------------------------------------------------------------
# Get API key from: https://api.together.xyz/
# Models: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo (default)
# Pros: Many model options, good pricing, reliable
# TOGETHER_API_KEY=your_together_api_key_here

# -----------------------------------------------------------------------------
# Ollama (Local Development - FREE)
# -----------------------------------------------------------------------------
# Install from: https://ollama.ai
# Models: mistral (default), llama3.2, phi3.5, qwen2.5
# Pros: Zero API costs, complete privacy, offline capable
# Run: ollama run mistral
# OLLAMA_BASE_URL=http://localhost:11434/v1

# -----------------------------------------------------------------------------
# AWS Bedrock (Enterprise/AWS Deployments)
# -----------------------------------------------------------------------------
# Requires: npm install @aws-sdk/client-bedrock-runtime
# Models: meta.llama3-70b-instruct-v1:0 (default)
# Pros: Enterprise features, AWS integration, provisioned throughput
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key
# AWS_REGION=us-east-1

# -----------------------------------------------------------------------------
# HuggingFace (Maximum Flexibility)
# -----------------------------------------------------------------------------
# Requires: npm install @huggingface/inference
# Get API key from: https://huggingface.co/settings/tokens
# Models: mistralai/Mistral-7B-Instruct-v0.2 (default)
# Pros: 350k+ models available, great for experimentation
# HUGGINGFACE_API_KEY=your_huggingface_api_key_here

# =============================================================================
# AWS S3 STORAGE (Optional - for file uploads)
# =============================================================================
# Use local storage or S3 for document uploads
USE_S3_STORAGE=false

# S3 Configuration (only needed if USE_S3_STORAGE=true)
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_REGION=us-east-1
AWS_S3_BUCKET=your-bucket-name

# =============================================================================
# DEPLOYMENT RECOMMENDATIONS
# =============================================================================
#
# Local Development:
#   AI_PROVIDER=ollama
#   - Free, no API costs
#   - Install: brew install ollama (Mac) or from ollama.ai
#   - Run: ollama run mistral
#
# Vercel Deployment:
#   AI_PROVIDER=groq
#   GROQ_API_KEY=your_key
#   - Fastest, serverless-friendly
#   - Free tier available
#   - Best for Vercel's request limits
#
# AWS Deployment:
#   AI_PROVIDER=bedrock
#   AWS_ACCESS_KEY_ID=your_key
#   AWS_SECRET_ACCESS_KEY=your_secret
#   - Seamless AWS integration
#   - Enterprise features
#   - Provisioned throughput for cost savings
#
# Budget-Conscious Production:
#   AI_PROVIDER=together
#   TOGETHER_API_KEY=your_key
#   - Good balance of quality and cost
#   - Many model options
#   - Reliable performance
